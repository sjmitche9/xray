# config/config.yaml

# dataset config
DATASET:
  ROOT_DIR: "Z:/mimic-cxr-jpg/files"
  METADATA_PATH: "data/mimic-cxr-jpg/mimic-cxr-2.0.0-metadata.csv"
  SPLIT_PATH: "data/mimic-cxr-jpg/mimic-cxr-2.0.0-split.csv"
  OUTPUT_PATH: "data/processed_dataset/fullset" # use /subset for testing and debugging (100 images)
  LATENT_OUTPUT_PATH: "data/processed_dataset/latent"
  IMAGE_SIZE: 256
  MAX_REPORT_LENGTH: 128
  LIMIT_SAMPLES: null  # set to null or a number like 100 for quick tests
  NUM_CPUS: 8
  STORE_SCALED_LATENTS: true

# model config
MODEL:
  DIFFUSION_CHECKPOINT: "checkpoints/diffusion.pt"
  DIFFUSION_RESUME: false
  TOKENIZER_NAME: "emilyalsentzer/Bio_ClinicalBERT"
  BASE_CHANNELS: 4
  UNET_BASE_CHANNELS: 32
  LATENT_DIM: 4
  LATENT_H: 32
  LATENT_W: 32
  VAE_CHECKPOINT: "checkpoints/vae.pt"
  VAE_RESUME: false
  LORA_CHECKPOINT: "checkpoints/lora_unet/"
  LORA_RESUME: true
  LORA_RESUME_LOAD_OPT_STATE: false
  LATENT_SCALE: 0.18215

# training config
TRAINING:
  BATCH_SIZE: 2
  GRAD_ACCUM_STEPS: 128
  MAX_GRAD_NORM: 1.0
  EPOCHS: 200
  LEARNING_RATE: 2e-5
  TEXT_TOKENS: 16 # this is now saved with the latent dataset
  LORA_R: 64
  LORA_ALPHA: 64
  LORA_DROPOUT: 0.05
  CFG_DROPOUT: 0
  GUIDANCE_SCALE: 3
  BETA: 1
  SSIM_WEIGHT: 0
  KL_MAX_WEIGHT: .1
  KL_ANNEAL_EPOCHS: 10
  WARMUP_EPOCHS: 20
  EARLY_STOP_PATIENCE: 50
  MIN_SAVE_EPOCH: 3
  TRAIN_ON_THREE_BATCHES: false # use this for debugging
  CHUNK_LIMIT: 10 # this limits the number of chunks loaded and trained on (max 36)

  LR_SCHEDULER:
    TYPE: "ReduceLROnPlateau"
    PATIENCE: 2
    FACTOR: 0.5
    MIN_LR: 1e-6

SCHEDULER:
  SAMPLING_STEPS: 1000

# logging
WANDB:
  PROJECT: "xray"
  RUN_NAME_VAE: "train_vae"
  RUN_NAME_DIFFUSION: "train_diffusion"
  RUN_NAME_LORA_UNET: "train_lora_unet"