# config/config.yaml

# dataset config
DATASET:
  ROOT_DIR: "Z:/mimic-cxr-jpg/files"
  METADATA_PATH: "data/mimic-cxr-jpg/mimic-cxr-2.0.0-metadata.csv"
  SPLIT_PATH: "data/mimic-cxr-jpg/mimic-cxr-2.0.0-split.csv"
  OUTPUT_PATH: "data/processed_dataset/fullset" # use /subset for testing and debugging (100 images)
  LATENT_OUTPUT_PATH: "data/processed_dataset/latent"
  IMAGE_SIZE: 256
  MAX_REPORT_LENGTH: 128
  LIMIT_SAMPLES: null  # set to null or a number like 100 for quick tests
  NUM_CPUS: 8

# model config
MODEL:
  DIFFUSION_CHECKPOINT: "checkpoints/diffusion.pt"
  DIFFUSION_RESUME: false
  TOKENIZER_NAME: "emilyalsentzer/Bio_ClinicalBERT"
  BASE_CHANNELS: 4
  UNET_BASE_CHANNELS: 32
  LATENT_DIM: 4
  LATENT_H: 32
  LATENT_W: 32
  VAE_CHECKPOINT: "checkpoints/vae.pt"
  VAE_RESUME: false
  LORA_CHECKPOINT: "checkpoints/lora_unet"
  LORA_RESUME: false

# training config
TRAINING:
  BATCH_SIZE: 8
  GRAD_ACCUM_STEPS: 8
  MAX_GRAD_NORM: 1.0
  EPOCHS: 100
  LEARNING_RATE: 1e-5
  LORA_R: 4
  LORA_ALPHA: 16
  LORA_DROPOUT: 0.1
  CONTEXT_DROPOUT_PROB: 0.1
  GUIDANCE_SCALE: 5
  BETA: 1
  SSIM_WEIGHT: 0.0
  KL_MAX_WEIGHT: .1
  KL_ANNEAL_EPOCHS: 10
  WARMUP_EPOCHS: 20
  EARLY_STOP_PATIENCE: 50
  MIN_SAVE_EPOCH: 3
  TRAIN_ON_THREE_BATCHES: true # use this for debugging
  CHUNK_LIMIT: 1 # this limits the number of chunks loaded and trained on

  LR_SCHEDULER:
    TYPE: "ReduceLROnPlateau"
    PATIENCE: 3
    FACTOR: 0.75
    MIN_LR: 1e-7

SCHEDULER:
  SAMPLING_STEPS: 250

# logging
WANDB:
  PROJECT: "xray"
  RUN_NAME_VAE: "train_vae"
  RUN_NAME_DIFFUSION: "train_diffusion"
  RUN_NAME_LORA_UNET: "train_lora_unet"